#+TITLE: Haskell and Realistic Single-Server Raster Processing
#+DATE: 2018-04-28
#+AUTHOR: Colin
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../assets/org-theme.css"/>

Alternate titles:

- Haskell and the Dream of Single-Server Raster Processing
- Consider Haskell for GIS

* Preface

Some problems that have historically plagued raster processing systems are:

- not being able to fit large images into memory
- not being able to index the entire image (i.e. more pixels than a 32-bit ~int~ type)
- burning time performing array algorithms serially
- intermediate memory allocation between operations

Long-running satellite imagery programs like [[https://landsat.usgs.gov/][Landsat]] and [[https://sentinel.esa.int/web/sentinel/home][Sentinel]] continue to
produce large imagery, so these problems remain relevant to modern systems.
In this article I announce my new Haskell library [[https://hackage.haskell.org/package/mapalgebra][mapalgebra]] and offer a
(hopefully) fair comparison with existing solutions.

* New Library: ~mapalgebra~

[[https://hackage.haskell.org/package/mapalgebra-0.1.0][mapalgebra]] is an implementation of /Map Algebra/ as defined by C. Dana Tomlin in
his classic text /GIS and Cartographic Modeling/. It is built on the fantastic
Parallel Array library [[https://hackage.haskell.org/package/massiv][massiv]] by Alexey Kuleshevich, and features op fusion
(i.e. "lazy Rasters"), flexible Local Operations, extremely fast parallel
Focal Operations, optional typesafe NoData handling, and the ability to
process arbitrarily large Rasters on a single machine.

Zonal Operations are planned but not yet implemented in ~v0.1~. Other aspects of
GIS work outside of Map Algebra are not provided.

** An Aside: The Haskell Language

[[https://www.haskell.org/][Haskell]] was first released in 1990, and is a strongly-typed, lazy, purely functional language.
These words mean specific things:

- *Strongly-typed:* All values have explicit types and the compiler is very strict
  about enforcing typing rules
- *Lazy:* Unneeded calculations are not performed
- *Purely:* Functions cannot mutate shared inputs nor affect global program state
- *Functional:* The fundamental unit of program composition is the function, and
  these can be passed to or returned from other functions

To further differentiate Haskell from Object-Oriented languages (like Java) or
Imperative ones (like C or Go), Haskell has no classes, loops, or mutable variables.
Despite that, we are still able to write anything we would want to write, and
the process is enjoyable. Haskell also boats [[https://hackage.haskell.org/][a vast library ecosystem]] and
[[https://docs.haskellstack.org/en/stable/README/][excellent build tooling]].

If you are unfamiliar with Haskell, [[https://www.youtube.com/watch?v=02_H3LjqMr8][this video by Derek Banas]] serves as a good introduction.
To learn it in full, consider [[http://haskellbook.com/][The Haskell Book]] by Chris Allen and Julie Moronuki. It starts
at the foundations and offers no shortcuts.

To try or develop Haskell, the book (and I) recommend [[https://docs.haskellstack.org/en/stable/README/][the Stack tool]].

** Features

~mapalgebra~'s answer to the Raster processing problem? /Be lazyâ„¢ and parallelize everything./

*** Type Safety

The fundamental type provided by ~mapalgebra~ is the ~Raster~. Its full
type signature can look like this:

#+BEGIN_EXAMPLE
  Raster S WebMercator 256 256 Int
         |  |          |    |   |
         |  |          |    |   ---> Data type of each pixel
         |  |          |    ---> Raster columns
         |  |          ---> Raster rows
         |  ---> Projection of the Raster
         ---> Current "memory mode".
              `S` stands for the `Storable` typeclass, meaning
              that this Raster is currently backed by a real
              in-memory byte-packed Array.
              This is either a freshly read image, or the result
              of computing a "lazy" Raster.
#+END_EXAMPLE

... or this ...

#+BEGIN_EXAMPLE
  Raster D LatLng 512 512 Word8
         |                |
         |                ---> A byte.
         ---> A "delayed" or "lazy" Raster.
              Likely the result of some Local Operation.
#+END_EXAMPLE

... or even this.

#+BEGIN_EXAMPLE
  Raster DW p 65000 65000 (Maybe Double)
         |  |       |      |
         |  |       |      ---> Rasters are fully polymorphic
         |  |       |           and can hold any data type.
         |  |       ---> Huge!
         |  ---> `p` is fine if you don't care about Projection.
         ---> A "windowed" Raster, the result of a Focal Op.
#+END_EXAMPLE

So why all the type variables? Why isn't ~Raster Int~ sufficient?
And why are there /numbers/ in a type signature?

~mapalgebra~ holds this as one of its fundamental assumptions:

#+BEGIN_QUOTE
Rasters of different size or projection are completely different types.
#+END_QUOTE

Unless your images have the same size and projection (or you've manipulated them
so that they do), you can't perform Local Operations between them. This eliminates
an entire class of bugs, and reduces complexity overall.

The numbers in the signatures are thanks to Haskell's ~DataKinds~ extension,
which lets us promote values into types for the purposes of type checking.

*** Lazy Rasters

I mentioned ~Raster S~ above as being backed by real memory. In contrast,
~Raster D~ is a "delayed" or "lazy" Raster. Operations over/between lazy Rasters
are always /fused/ - they don't allocate additional memory between each step.
Consider [[https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index][NDVI]]:

#+BEGIN_SRC haskell
  -- | Doesn't care about the projection or Raster size, so long as they're
  -- all the same.
  ndvi :: Raster D p r c Double -> Raster D p r c Double -> Raster D p r c Double
  ndvi nir red = (nir - red) / (nir + red)
#+END_SRC

Or the [[https://en.wikipedia.org/wiki/Enhanced_vegetation_index][Enhanced Vegetation Index]] calculation:

#+BEGIN_SRC haskell
  -- | The constants are interpreted as lazy Rasters of only that value.
  evi :: Raster D p r c Double -> Raster D p r c Double -> Raster D p r c Double -> Raster D p r c Double
  evi nir red blue = 2.5 * (numer / denom)
    where numer = nir - red
          denom = nir + (6 * red) - (7.5 * blue) + 1
#+END_SRC

Here we see 8 binary operations being used, but none of them perform calculations or
allocate new memory (yet). This saves /a lot/ of time that would otherwise be spent
iterating multiple times through the Array.

Only the application of the ~strict~ function on a
~Raster D~ or ~Raster DW~ will actually run anything and allocate a new underlying Array.
For the purposes of GIS, that Array could have type ~S~ (Storable, for imagery IO.
Primitive types only) or ~B~ (Boxed, for custom data types).

*** Free Parallelism

Good news: so long as you compile with ~-with-rtsopts=-N~, code that uses
~mapalgebra~ will automatically utilize all of your CPUs for its calculations.
No other special configuration, custom code, or developer overhead is required.

For production systems, this means informally that the more CPUs you throw
at the problem, the faster it will get "for free". Please do take [[https://en.wikipedia.org/wiki/Amdahl%27s_law][Amdahl's Law]]
into account, though.

** Performance

Decades of work have gone into GHC, the main Haskell compiler, and it can produce
highly optimized machine code.
Thanks to this, to [[https://hackage.haskell.org/package/massiv][Massiv]], to some rewrites of mine, and to a lot of benchmarking, I've achieved
speeds that are competitive-or-better than existing libraries in the field.
[[https://docs.google.com/spreadsheets/d/1XubgdBGYEMnUNfaNvEYmp8KCfyra77_n1sx0Ej5hgU8/edit#gid=0][Documenting the entire process]] helped greatly to prove which (and how) changes
were useful.

Haskell also has an [[https://llvm.org/][LLVM]] backend, which can be accessed with the ~-fllvm~ compiler flag.
Not all operations benefit from it, but the ones that do gain about a 2x speed-up.

A full chart of benchmarks is available below in the /Benchmarks/ section.

** Short-comings

~mapalgebra~ is just that - a library for Map Algebra. It will not cook you
dinner nor do your taxes. If you need a more fully featured GIS
suite, please consider GeoTrellis.

There are benefits to having a focus, of course. Even so, here are some reasons
why you might want to avoid ~mapalgebra~ until future versions.

*** Projections aren't read at IO time

With ~v0.1~, projection information isn't yet read out of imagery
and enforced statically like size is. This means that using the ~p~ parameter
is a "best practice". If you mark an image as being ~WebMercator~ and it's
actually ~LatLng~... well, best of luck.

*How could this be fixed?* Tiff metadata reading is a planned feature for
Massiv, the library upon which ~mapalgebra~ is built. Once that is complete,
then Projection can be enforced like size.

*** Imagery size must be known ahead of time

This is by design, but I could see there being claims that it's inconvenient
for live systems.

*How could this be fixed?* There are two paths I can see:

1. Double down. Make a judgement that imagery size is a data sanitation problem,
   and that all imagery of differing size should be transformed before being
   fed through ~mapalgebra~.

2. Provide a function like:

#+BEGIN_SRC haskell
  conformingRead :: FilePath -> IO (Raster S p r c a)
#+END_SRC

which, when annotated with the projection and dimensions you want will automatically
reproject and up/downsample if necessary when imagery is read. That way, the
"data sanitation gate" is held at the IO boundary, and all of your Map Algebra will be rigourous.

*** Incomplete NoData handling

Say you have this image:

[[./blog/nodata.jpg]]

Lots of NoData, which is pretty common for Landsat imagery. What happens if we naively
run a Focal Operation over it? Currently, Focal Ops only support a 3x3 square neighbourhood,
so not much would go wrong: our data/nodata border pixels might accrue artifacts.
This would be especially pronounced with an ~Int~ cell type and ~-2^63~ as the NoData value.

One way to handle the NoData in a typesafe way is via ~Maybe~:

#+BEGIN_SRC haskell
  import Data.Monoid (Sum(..))

  -- | `Maybe` has a `Monoid` instance, which by default ignores any `Nothing`
  -- that are added to it. `fmonoid` is used to smash the neighbourhood together.
  nodatafmean :: Raster S p r c Word8 -> Raster DW p r c 512 Word8
  nodatafmean = fmap (maybe 0 getSum) . fmonoid . strict B . fmap check . lazy
    where check 0 = Nothing
          check n = Just $ Sum n
#+END_SRC

The problem is that due to the ~strict B~, a boxed vector is allocated which slows
this operation down quite a bit. So, which'll it be? Correctness or speed?

*How could this be fixed?* We have a few options:

1. Give up and say "that's the cost of type safety".
2. Declare that NoData is also a data sanitization problem and that all imagery
   should have its NoData interpolated or removed before being ran through
   ~mapalgebra~.
3. Give ~Maybe~ an instance of ~Storable~ somehow, so that ~strict S~ can be used
   instead.
4. Investigate [[https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#unboxed-sums][Unboxed Sums]] more deeply.

*** No reprojection support

There is a ~Projection~ typeclass, but instances have not been written for the
various types.

*How could this be fixed?* Do some research and write them, lazy bones.

*** Slow ~faspect~ and ~fgradient~

~fvolume~, ~fupstream~, and ~fdownstream~ are quite fast due to some math tricks.
Their angle-oriented siblings, however, had no such shortcuts that I knew of.
I call out to another library, ~hmatrix~, to handle the linear algebra ops
described [[https://hackage.haskell.org/package/mapalgebra-0.1.0/docs/Geography-MapAlgebra.html#g:15][in the module documentation]], and something about this undoes the
usual efficiency of windowed Rasters.

*How could this be fixed?* Are their better approaches to ~faspect~ and ~fgradient~?
Maybe I should be using a [[https://hackage.haskell.org/package/linear][different Linear Algebra library]].

*** No extended neighbourhood for Focal Ops

This is a non-trivial thing to leave out, but honestly I didn't have the
resources to include this in an initial version of the library.

If you need extended or non-square neighbourhoods, please consider GeoTrellis
[[https://geotrellis.github.io/scaladocs/latest/#geotrellis.raster.mapalgebra.focal.Neighborhood][which has a variety]].

*How could this be fixed?* Perhaps for each focal op, say ~fmean~, I could
provide a variant ~fmeanWith~ that takes an arbitrary stencil. I'd also have
to provide some stencil making functions, so that it would be easy to, for instance,
generate a 10x10 square on a whim.

*** No true Multiband support

~mapalgebra~ and Massiv rely on [[http://hackage.haskell.org/package/JuicyPixels][JuicyPixels]] for image IO. At present, it can only
read TIFFs in RGBA mode (or simpler). True multibanded imagery, like that from Landsat,
just can't be read.

*How could this be fixed?* We already have the ~RGBARaster~ wrapper type, but that's not
enough. I need to either:

1. Tell JuicyPixels about my needs (or submit a patch).
2. Write my own library for specifically reading multiband geotiffs.

Either way, ~mapalgebra~ takes the stance that multiband imagery, as a special data type,
doesn't exist. A multiband image is just a collection of singleband images, which are
each just a ~Raster~.

*** Potentially long compile times

In order to be performant, ~mapalgebra~ relies on ~-O2~ and a lot of inlining.
Depending on the amount of calls to its functions, this can really slow down
compilation of your code.

*How could this be fixed?* That's life, I'm afraid. When testing, make sure
to compile with ~stack build --fast~ so that all optimizations are skipped.
It really makes a difference.

*** Why not GPU?

If going single-machine, why not do it "right" and use a giant GPU instead?
I can't find a rebuttle for that, other than that CPUs seem more readily available.
This is somewhat of an existential issue for ~mapalgebra~ - for it to have value
we'd need to agree that CPU-based Raster processing is useful. Is it?
I hope so.

* What about Rasterio (Python)?

** The Strengths of Rasterio

*** Numpy

Fast fast, thanks to C.

*** Mindshare

Many people already know how to use Python.

** The Weaknesses of Rasterio

*** Dynamic Typing

*** No Focal Operations

* What about GeoTrellis (Scala)?

[[https://geotrellis.io/][GeoTrellis]] is a feature-rich GIS library suite for Scala ([[http://geotrellis.readthedocs.io/en/latest/][docs]]). It can process raster,
vector (geometric), vector tile, and point-cloud data, and has integration with
[[http://geotrellis.readthedocs.io/en/latest/guide/tile-backends.html][many popular storage systems]], like S3 and Cassandra.

*Disclaimer:* I used to work for the team that created and maintains GeoTrellis.
Were it not for their wisdom and support, I would not be where I am today. I thank
them and all of [[https://www.azavea.com/][Azavea]] for everything they taught me.

Any criticism of GeoTrellis here is made with the utmost respect for my former colleagues and
their continuing effort.

** Philosophy and Prime Use-Cases

GeoTrellis' answer to the Raster processing problem? /Distribution./

** Short-comings

*** Opaque ~Tile~ Types

Link to the Tile hierarchy image.

*** Mixed Projections and Raster Sizes

I'd be a rich man if I had a nickle for every time results were incorrect because
of faulty assumptions about what Projection our data was in.

*** Lack of Op Fusion

Talk about EVI.

*** API Discoverability

GeoTrellis utilizes around 15 actual data types (~Tile~, ~Polygon~, etc.).
Despite this, its scaladocs (TODO link!) present users with XXX top-level symbols.
XXX of these are method-injection boilerplate which permit the following:

#+BEGIN_SRC scala
  // Given...
  val tile: Tile = ???

  // Some example of manually calling `focalMean`

  // ... we can do this. Much more idiomatic!
  val averaged: Tile = tile.focalMean
#+END_SRC

For modularity reasons ~.focalMean~ isn't defined directly within the ~Tile~
class. This is good because it keeps the ~Tile~ class clean. What isn't good
is the resulting boilerplate: the XXX top-level symbols which a user doesn't need
to see but is shown anyway.

Another downside is that ~.focalMean~ doesn't appear in Tile's scaladocs entry.
The question "what can I do to a ~Tile~?" is not easy to answer. This situation
comes up often too:

#+BEGIN_QUOTE
I want to turn a ~Tile~ into a ~Foo~. What do I need to import and what
injected method do I need to call for that?
#+END_QUOTE

The pursuit of that answer has burnt a lot of user time over the years. The
worst case scenario is when they need to ask us directly...
Something is wrong when a library's basic functionality is not self-explanitory.

*How could this be fixed?*

*** Overuse of Spark

** A Word on Scala, in General

Everyone is entitled to their favourite language, and with the right training
and motivation, good work can be done in any of them.
Hell, the software for the [[https://en.wikipedia.org/wiki/New_Horizons][New Horizons]] satellite was written in Assembly.
That said, any group of programmers would probably agree that in general, some
languages are better than others. What better /means/ is hard to qualify, though.
Because it's hard, to swing to the opposite extreme and say "all languages
are equally good" is likely more politics than anything, so let's try our best.

We could measure language quality along these axes:

- Expressivity
  - Some "power"-vs-characters-typed ratio.
  - Related to Verbosity, with Assembly on one extreme and [[https://en.wikipedia.org/wiki/APL_(programming_language)][APL]] on the other.
- Aethetics
  - I know I've been visually offended by my own code before.
- Library Ecosystem
  - Languages are born and die all the time for lack of users.
- Performance
  - Specifically: is /idiomatic/ use of the language performant, or is the
    language filled with...
- GOTCHAs
  - Not just things that confuse learners - things that also confuse and frustrate
    experienced users.
- Risk of Verbosity and Accidental Complexity
  - Given an otherwise capable dev, how easy is it to accidentally write
    complex code?
- Beginner-centricness
  - How long does it take to become productive?
  - Too much or too little of this are both bad.
- Tooling
  - Is there a "blessed" editor for the language?
  - What is the error feedback mechanism?
    (Hopefully not "run it and see what fails")
  - How are projects defined and built in the language?
  - Does "dependency hell" exist in the language?
- Core Development Team
  - Are the language's core devs qualified with a good track record?
  - How do the core devs handle feedback / criticism?
- Community
  - Are established members willing to teach? Are newcomers willing to learn?
  - Where and how do members congregate and communicate?
  - How political is the community?
- Momentum
  - When was the language's last major release?
  - Is there a huge user base? A brain-drain to other languages? Should we care?
- Learning Resources
  - Are there books? Talks? Courses?

No single axis should sway a decision to use a language. Hopefully a language's
strong axes outweight the weak enough to maintain dev sanity.

There is a meta-axis too which generalizes everything above: Order vs Chaos.
Javascipt? Chaotic, but you can force Order upon it with best practices. Haskell?
Ordered, but you can introduce Chaos into it with care.

Having written Scala professionally for 4 years, I would rank it at least
"below average" on
almost every axis described above, and call it more Chaotic than Ordered.
For GIS in particular, its use should be avoided. I'll elaborate
on the points which are relevant to GIS (and/or Data Science) below.

*** Performance GOTCHAs and Lack of Optimizations

~scalac~ is not an optimizing compiler. There is no ~-O2~.
We can't write clean, idiomatic code
and expect the compiler to "do the right thing".
[[https://github.com/fosskers/scala-benchmarks][Naive use of the standard library]] also can't be trusted to perform well,
even with the JIT (except tail recursion over ~List~, which rivals
~while~ loops for speed).

To achieve "performance
Scala" you must bend the language to your will, using loops, mutable variables,
and mutable Java collections.

*How could this be fixed?* [[https://github.com/scala-native/scala-native][Scala Native]] is a Scala dialect which compiles to
LLVM, which /does/ optimize. Unfortunately, it isn't production ready
and only supports a subset of the standard Java libraries. Scala libraries also
need to be specifically built for it.

*** Verbosity and Accidental Complexity

It has been my experience that Haskell code is usually 2 to 3 times shorter than Scala
for equivalent functionality. ~mapalgebra~ has around 600 lines of code (not counting
comments). The total line count of all modules from GeoTrellis that implement the "same"
functionality is around 6,000. Such surface area is bound to breed bugs, and require
more than one human to manage it. If such code lengths are typical in Scala codebases,
what could labour costs be reduced to if this wasn't the case?

*How could this be fixed?* I don't know that it can be. Scala would have to be a
different language altogether.

*** No Unsigned Primitives
*** Aggressive Boxing of Primitives
*** Binary Compatibility
*** Deployment Woes
* Benchmarks
* Resources

- GT site
- [[http://geotrellis.readthedocs.io/en/latest/][GeoTrellis Documentation]]
- Link to Rasterio paper
- ~mapalgebra~ hackage page
